start_time,end_time,speaker,title,abstract,bio,expandable,,,
9:30,10:00,Registration,,,,FALSE,,,
10:00,10:10,Katrina Ligett,Opening remarks,,,FALSE,,,
10:10,11:00,Uri Stemmer,How to Find a Point in the Convex Hull Privately and Efficiently?,"We study the question of how to compute a point in the convex hull of $n$ points in $\mathbb{R}^d$ in a differentially private manner. This question, which is trivial non-privately, turns out to be quite deep when imposing differential privacy. In particular, it is known that the input points must reside on a fixed finite grid G in  $\mathbb{R}^d$, and furthermore, the number of input points n must grow with the size of the grid $|G|$. \n Previous works focused on understanding how n needs to grow with $|G|$ and d, and showed that $n=poly(d, log^{*}|G|)$ suffices (so n does not have to grow significantly with $|G|$). However, all existing constructions require runtime at least $n^d$. In this paper, we present a differentially private algorithm with runtime $poly(n, d, log|G|)$. To achieve this result, we introduce efficient differentially private algorithms for several related tasks, including solving systems of linear equalities and inequalities. \n Joint work with Haim Kaplan, Yishay Mansour, Shay Moran, and Nitzan Tur.","Uri is a faculty member in the School of Computer Science at Tel Aviv University. Previously, he was a faculty member at Ben-Gurion University and a postdoctoral researcher at the Weizmann Institute of Science and Harvard University. He received his Ph.D. from Ben-Gurion University in 2017.",TRUE,,,
11:00,11:25,Tom Waknine,Agnostic Learning under Targeted Poisoning,"We study the problem of learning in the presence of an adversary that can corrupt an $\eta$ fraction of the training examples with the goal of causing failure on a specific test point. In the realizable setting, prior work has established the optimal error under such instance-targeted poisoning attacks. In the agnostic setting, it is known that using deterministic learners, an adversary can force the error to be arbitrarily close to 1. \n Our result resolves this problem in the agnostic setting by allowing randomized learners. I will present the tight bounds we derive in this setting and highlight the critical role that randomness plays in achieving them. Time permitting, I will also sketch the proof approach and walk through an illustrative toy example.","Tom is a PhD studnet at the technion under professor Shay moran. His main research interests include learning theory, combinatorics, and probability theory. His primary focus is on studying the fundamental principles underlying the theoretical foundations of machine learning",TRUE,,,
11:25,11:50,Odelia Melamed,Adversarial Examples - From Puzzling Experimental Findings to Theoretical Perspective and Applications,"The extreme fragility of deep neural networks, when presented with tiny perturbations in their inputs, was independently discovered by several research groups in 2013. However, despite enormous effort, these adversarial examples remained a counterintuitive phenomenon with no simple testable explanation. In this lecture, I will present a new conceptual framework for understanding adversarial examples through the lens of high-dimensional geometry, offering an intuitive mental image that aligns with empirical observations. We will then explore how this perspective translates into a simplified theoretical setting. Finally, I will demonstrate how this view can be used to analyze and simplify both adversarial attack strategies and robustness methods.","Odelia Melamed is a PhD student advised by Professor Adi Shamir at the department of Applied Mathematics and Computer Science, Weizmann Institute of Science, Israel. Her research interests include ML security, Adversarial Examples in Machine Learning, and Theoretical Machine Learning.",TRUE,,,
11:50,12:50,,Lunch,,,FALSE,,,
12:50,13:15,Guy Smorodinsky,Provable Privacy Attacks On Trained Shallow Neural Networks,"In recent years, neural networks have become deeply embedded in everyday applications, often trained on sensitive datasets such as financial or healthcare records. This widespread use raises critical concerns about data privacy, as malicious actors may attempt to extract information about the training data from a deployed model. A growing body of empirical work has demonstrated the feasibility of such attacks, including recent methods that exploit the fact that trained networks typically converge to a point satisfying a set of known constraints, enabling partial reconstruction of the training data. \n In this talk, I will present the first formal results showing that privacy attacks of this particular kind can provably succeed. In the first part, I will focus on data reconstruction attacks in a univariate setting, and in the second part, I will turn to membership inference attacks in high-dimensional scenarios. These results provide a theoretical foundation for understanding when and why privacy vulnerabilities arise in trained neural networks, highlighting the need for stronger privacy-preserving mechanisms.","Guy Smorodnisky is a Ph.D. student at Ben-Gurion University, supervised by Dr. Itay Safran. His research focuses on the theoretical foundations of deep learning, with a particular interest in privacy and security.",TRUE,,,
13:15,13:40,Sol Yarkoni,Low-Resource Reconstruction of Template-Memorized Images,"The recent advances in generative models such as diffusion models has raised several risks and concerns related to privacy, copyright infringements and data stewardship. To better understand and control the risks, various researchers have created techniques, experiments and attacks that reconstruct images, or part of images, from the training set. While these techniques already establish that data from the training set can be reconstructed, they often rely on high-resources, excess to the training set as well as well-engineered and designed prompts. \n In this work, we devise a new comprehensive attack that requires low resources, assumes little to no access to the actual training set, and identifies, seemingly, benign prompts that lead to potentially-risky image reconstruction. This highlights the risk that images might even be reconstructed by an uninformed user and unintentionally. For example, we identified that, with regard to one existing model, the prompt ``Abstract Art Unisex T-Shirt'' can generate the face of a real-life existing human (who we could identify by name). Our method builds on an intuition from previous works which leverages domain knowledge and identifies a fundamental vulnerability that stems from the use of scraped data from e-commerce platforms, where templated layouts and images are tied to pattern-like prompts.","Sol is a MSc student researching memorization in generative models. Before returning to academia, she worked as a data scientist specializing in computer vision, tackling a wide range of real-world challenges across various domains. Alongside her research, she is also a generative artist, exploring the creative potential of the same models she studies.",TRUE,,,
13:40,14:05,Liza Nesterova,Reconstruction and Secrecy under Approximate Distance Queries,"We study the task of reconstructing an unknown target point using approximate distance queries. In each round, the reconstructor selects a reference point and receives a noisy estimate of its distance to the target. This framework captures many settings—from GPS localisation and sensor networks to privacy-aware data access—and applies across a broad class of metric spaces. \n What is the smallest worst-case error the reconstructor can guarantee against any responder, assuming access to an unlimited number of queries? Our main result gives a tight geometric answer: for every compact (indeed, totally bounded) metric space, the optimal error equals the Chebyshev radius of the largest subset whose diameter is bounded by the noise-determined scale. This characterisation is both necessary and sufficient, and it yields explicit formulas in familiar spaces such as Euclidean spaces, spheres, and trees. \n This viewpoint opens several directions for future work. Having pinned down the optimal error, the next natural question is: How quickly can a reconstructor approach this bound?","Liza completed her Master’s degree at the Technion in 2024, with a thesis in algebraic geometry. Since December 2024, she have been pursuing a PhD at the Technion under the supervision of Shay Moran, focusing on learning theory.",TRUE,,,
14:05,14:25,,Coffee break,,,FALSE,,,
14:25,14:50,Haneen Najjar,Disparity Robustness and Fairness,,,TRUE,,,
14:50,15:15,Amit Keinan,How Well Can Differential Privacy Be Audited in One Run?,"Recent methods for auditing the privacy of machine learning algorithms have improved computational efficiency by simultaneously intervening on multiple training examples in a single training run. Steinke et al. (2024) prove that one-run auditing indeed lower bounds the true privacy parameter of the audited algorithm, and give impressive empirical results. Their work leaves open the question of how precisely one-run auditing can uncover the true privacy parameter of an algorithm, and how that precision depends on the audited algorithm. In this work, we characterize the maximum achievable efficacy of one-run auditing and show that the key barrier to its efficacy is interference between the observable effects of different data elements. We present new conceptual approaches to minimize this barrier, towards improving the performance of one-run auditing of real machine learning algorithms.","Amit Keinan is a master's student in computer science at the Hebrew University of Jerusalem under the supervision of Prof. Katrina Ligett. His research focuses on efficient methods for auditing differential privacy, with emphasis on their theoretical capabilities and limitations.",TRUE,,,
15:15,15:40,Noga Amit,Worst-Case Guarantees in an Average-Case World,"How can we trust the correctness of a learned model on a particular input of interest? Model accuracy is typically measured on average over a distribution of inputs, giving no guarantee for any specific input. This talk introduces Self-Proving models, a new class of models that formally prove the correctness of their outputs via an Interactive Proof system. We will formally define Self-Proving models and their per-input (worst-case) guarantees. We will then present algorithms for learning these models and explain how the complexity of the proof system affects the complexity of the learning algorithms. Finally, we will review experiments in which Self-Proving Models are trained to compute the greatest common divisor (GCD) of two integers and prove their correctness to a simple verifier.","Noga is a PhD student at UC Berkeley, advised by Prof. Shafi Goldwasser and co-advised by Prof. Guy Rothblum, who was also her master's advisor at the Weizmann Institute. She completed her undergraduate studies at Tel Aviv University, earning a BSc in Math and Computer Science. Her research focuses on cryptography and machine learning.",TRUE,,,
15:40,16:00,,Coffee break,,,FALSE,,,
16:00,16:25,Aryeh Kontorovich,On the tensorization of the variational distance,"If one seeks to estimate the total variation between two product measures $||P^{\otimes_{1:n}}-Q^{\otimes_{1:n}}||$ in terms of their marginal TV sequence $\delta=(||P_1-Q_1||,||P_2-Q_2||,\ldots,||P_n-Q_n||)$, then trivial upper and lower bounds are provided by$ ||\delta||_\infty \le ||P^\otimes_{1:n}-Q^\otimes_{1:n}||\le||\delta||_1$. We improve the lower bound to $||\delta||_2\lesssim||P^\otimes_{1:n}-Q^\otimes_{1:n}||$, thereby reducing the gap between the upper and lower bounds from $\sim n$ to $\sim\sqrt{n} $. Furthermore, we show that {\em any} estimate on $||P^\otimes_{1:n}-Q^\otimes_{1:n}||$ expressed in terms of $\delta$ must necessarily exhibit a gap of $\sim\sqrt n$ between the upper and lower bounds in the worst case, establishing a sense in which our estimate is optimal. Finally, we identify a natural class of distributions for which $||\delta||_2$ approximates the TV distance up to absolute multiplicative constants. \n This result has already found applications in private distribution learning. <a href=""https://projecteuclid.org/journals/electronic-communications-in-probability/volume-30/issue-none/On-the-tensorization-of-the-variational-distance/10.1214/25-ECP680.full"" target=""_blank"">Paper link</a>","Aryeh Kontorovich received his undergraduate degree in mathematics with a certificate in applied mathematics from Princeton University in 2001. His M.Sc. and Ph.D. are from Carnegie Mellon University, where he graduated in 2007. After a postdoctoral fellowship at the Weizmann Institute of Science, he joined the Computer Science department at Ben-Gurion University of the Negev in 2009, where he is currently a full professor. His research interests are mainly in machine learning, with a focus on probability, statistics, Markov chains, and metric spaces. He served as the director of the Ben-Gurion University Data Science Research Center during 2021-2022.",TRUE,,,
16:25,16:50,Yuval Dagan,Breaking the $T^{2/3}$ Barrier for Sequential Calibration,"A set of probabilistic forecasts is calibrated if each prediction of the forecaster closely approximates the empirical distribution of outcomes on the subset of timesteps where that prediction was made. We study the fundamental problem of online calibrated forecasting of binary sequences, which was initially studied by Foster & Vohra (1998). They derived an algorithm with O(T^{2/3}) calibration error after T time steps, and showed a lower bound of \Omega(T^{1/2}). These bounds remained stagnant for two decades, until Qiao & Valiant (2021) improved the lower bound to \Omega(T^{0.528}) by introducing a combinatorial game called sign preservation and showing that lower bounds for this game imply lower bounds for calibration. \n In this paper, we give the first improvement to the $O(T^{2/3})$ upper bound on calibration error of Foster & Vohra. We do this by introducing a variant of Qiao & Valiant's game that we call sign preservation with reuse (SPR). We prove that the relationship between SPR and calibrated forecasting is bidirectional: not only do lower bounds for SPR translate into lower bounds for calibration, but algorithms for SPR also translate into new algorithms for calibrated forecasting. We then give an improved \emph{upper bound} for the SPR game, which implies, via our equivalence, a forecasting algorithm with calibration error $O(T^{2/3 - \varepsilon})$ for some \varepsilon > 0, improving Foster & Vohra's upper bound for the first time. Using similar ideas, we then prove a slightly stronger lower bound than that of Qiao & Valiant, namely \Omega(T^{0.54389}). Our lower bound is obtained by an oblivious adversary, marking the first $\omega(T^{1/2})$ calibration lower bound for oblivious adversaries. \n Joint work with Constantinos Daskalakis, Maxwell Fishelson, Noah Golowich, Robert Kleinberg and Princewill Okoroafor","Yuval Dagan is a Senior Lecturer in the School of Computer Science at Tel Aviv University. He received his B.Sc. and M.Sc. in Computer Science from the Technion, where he was advised by Prof. Yuval Filmus. He completed his Ph.D. in Electrical Engineering and Computer Science at MIT under the supervision of Prof. Constantinos Daskalakis, during which he was awarded the Meta Research Ph.D. Fellowship. His doctoral work was recognized with the George M. Sprowls Ph.D. Thesis Award in Artificial Intelligence and Decision Making, awarded by MIT. Before joining Tel Aviv University, he was a postdoctoral researcher at the Simons Institute for the Theory of Computing at UC Berkeley.",TRUE,,,
16:50,17:00,Katrina Ligett,Closing remark,,,FALSE,,,